## 要約
我々は、キーバリューキャッシュに直接適用されるワンショット介入による言語モデルの暗黙的誘導を実現する軽量手法「キャッシュステアリング」を提案する。その有効性を検証するため、小規模言語モデルにおいて思考連鎖推論を誘導するためにキャッシュステアリングを適用した。本手法はGPT-4oが生成した推論トレースを活用し、微調整やプロンプト変更なしにモデルの挙動をより明示的で多段階の推論へと導く誘導ベクトルを構築する。多様な推論ベンチマークにおける実験評価により、キャッシュステアリングが
モデル推論の質的構造と定量的タスク性能の両方を向上させることが実証された。
継続的な介入を必要とする従来の活性化ステアリング技術と比較して、
我々のワンショットキャッシュステアリングは、ハイパーパラメータの安定性、
推論時間の効率性、統合の容易性の面で大きな利点を提供し、
制御された生成のためのより堅牢で実用的な解決策となる。

## はじめに
大規模言語モデルが複雑な推論を実行する能力は、その有用性が高まる主要な要因である。しかし、この潜在能力は常に自発的に発揮されるわけではなく、特に潜在的な推論能力を有しながらも特定の誘導を必要とする小規模モデルでは顕在化しない。教師あり微調整や思考連鎖例を用いた少例学習プロンプティングといった従来の能力発掘手法は有効だが、膨大なデータや複雑なプロンプト設計を要することが多い。そこで疑問が生じる：トレーニング後にこれらの固有推論プロセスを解き放ち誘導する、より軽量な介入手法を開発できるだろうか？

有望な方向性の一つが活性化誘導（Turner et al., 2024; Panickssery et al., 2023）である。これはモデルの内部隠れ状態を直接変更することで、その挙動を導くことを目的とする。再学習なしで出力に影響を与え得る点で有望である一方、活性化誘導は効果を発揮するためにデコードプロセス全体を通じて各トークン生成ステップで継続的な介入を必要とする場合が多い（Wehner et al., 2025）。この継続的な操作は不安定性を生じさせ、結果を超パラメータの選択（対象とする層、介入強度など）に極めて敏感にし、生成品質の低下を招く可能性がある。

これらの課題に対処するため、我々は「キャッシュステアリング」と呼ばれる手法を導入する。本アプローチは、Transformerモデルのキーバリューキャッシュに対して直接、標的を絞った単発の修正を行うことで動作する。
通常、キャッシュは最初のプロンプトによって初期化されます。GPT-4oのような高性能な教師モデルが生成した推論トレースから導出されたステアリングベクトルを、これらのキャッシュされたキーと値の表現に適用することで、小型モデルの推論軌道を誘導できます。この単一の介入は、トークン生成開始前に適用されるため、モデル重みを変更したり複雑なプロンプト修正を必要とせずに、より明示的で多段階の推論へと効果的に誘導します。

これにより以下の主要な利点が得られる：
ハイパーパラメータ変動に対する安定性と頑健性の向上、
継続的な介入が不要なため推論時の計算オーバーヘッドの大幅削減、
標準的なトランスフォーマー推論パイプラインとのシームレスな統合。
我々の手法は推論構造を改善し、GSM8K、ARC-Challenge、CSQA、PIQAを含む
複数のベンチマークにおいて多くのケースでタスク精度を向上させることを実証する。
全体として、我々の主な貢献は以下の通りである：
• キャッシュステアリングを提案する。これは推論時にKVキャッシュをワンショットで変更することでモデル挙動を制御する新規技術である。
• キャッシュステアリングにより、GPT-4oなどの大規模モデルから推論スタイルを蒸留し、
微調整やプロンプト拡張なしに小型モデルへ転移できることを実証。
• 推論ベンチマークで広範な評価を実施し、活性化ステアリングやCoTプロンプティングと比較。
効率性と安定性を分析。

## 関連研究
### 推論と思考の連鎖プロンプティング。
LLMの推論能力を強化する広く採用されている手法には、言語モデルへのプロンプト内で段階的な推論プロセス（思考の連鎖：CoT）を含む問題の解決例を示す方法（文脈内学習：ICL）がある（Brown, 2020; Wei et al., 2022）。この手法は「少例学習プロンプティング」として知られる。
CoTプロンプティングのゼロショット変種は、「段階的に考えよう」といった指示を追加することで、
例示の提示を必要とせずに段階的推論を誘導し、このアプローチを簡略化する（Kojima et al., 2022）。
最近の研究では、強化学習が顕著な推論能力をもたらし、
教師あり微調整を通じて小型モデルへ効果的に蒸留できることが示されている（Guo et al., 2025）。
これらの知見は、言語モデルでCoT推論を単に誘発するだけでは不十分であり、推論の様式が重要であることを示唆している。これが我々のアプローチの動機であり、キャッシュレベルでの介入を通じて、大規模な教師モデルを彷彿とさせる推論行動へ小型モデルを直接誘導することを目指す。

### 活性化ステアリング。
活性化ステアリング（別名：表現工学）は、通常は線形介入を通じて、デコード中の中間活性化を操作することでLLMの生成プロセスを暗黙的に制御する技術である（Panickssery et al., 2023; Turner et al., 2024）。複数の研究が、再学習なしにモデルに特定の行動を誘導または抑制するために活性化ステアリングを適用している。例としては、感情・トピック・スタイル制御（Turner et al., 2024）、機能ステアリング（Todd et al., 2023; Postmus and Abreu, 2024）、拒否行動の除去または誘導、 （Lee et al., 2024）、有害性の低減（Turner et al., 2024）、真実性（Wang et al., 2025）、
事実知識の編集（Yin et al., 2024）、推論（Zhang and Viteri, 2025; Galichin et al., 2025)
その他（Wehner et al., 2025）などである。
最も基本的な形態において、活性化ステアリングは2つのステップを含む：ベクトル抽出と推論時のモデル活性化へのベクトル注入。ベクトル抽出段階では「ステアリングベクトル」を計算する。これは通常、望ましい挙動を持つ肯定的プロンプトと否定的あるいは中立的なプロンプトのペアから活性化を集約することで行われ、対照セットt$C = {(p^+_0, p^−_0),(p^+_1, p^−_1), ...,(p^+_N , p^−_N)}$を形成する。最も一般的な集約手法はDifferencein-Means（Wehner et al., 2025）であり、ベクトルがペア化されている場合にはMean-of-Differencesと同一となる：
$$s_l = \frac{1}{N} \sum_{(p^+,p^−)∈C} f_l(p^+) − f_l(p^−)$$

ここで、fl は層 l における Transformer モデルの一部（例：デコーダ層全体）を表し、
N は対照データセット C 内の例の数である。モデルの出力の方向を制御するため、推論中に特定の層の活性化値にステアリングベクトルを加算する：
$$h^∗_l = h_l + c s_l$$
ここで hl はステアリング前の層 l における活性化値、sl は層 l から抽出されたステアリングベクトル、c はステアリングの強さを決定する係数である。
重要な点として、このベクトルは異なるトークン位置、層、モデル部分に対して抽出・適用可能であり、これらはハイパーパラメータまたは設計上の選択として扱われる。通常、ステアリングを適用する層とステアリング強度係数 c の値を決定するにはグリッド検索を行うのが一般的である（Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024）。
活性化ステアリングはモデル制御の手段を提供する一方で、生成中に継続的な介入を必要とするのが一般的である（Wehner et al., 2025）。これはコストが高く、不安定な生成を引き起こす可能性がある。これに対し、我々の研究は活性化ステアリングの原理をキーバリュー（KV）キャッシュに拡張し、推論時に効率的かつ安定性の高いワンショット介入を可能にする。

### キャッシュ操作。
別の新たな研究分野では、メモリと効率性の観点からキーバリュー（KV）キャッシュを改変するアイデアが探求されている（Li et al., 2024; Liu et al., 2024a; Ge et al., 2023; Mu et al., 2023）。これらのアプローチは、KVキャッシュ操作を通じてメモリフットプリントの削減や文脈表現の圧縮を目指す。この概念を発展させ、Liu et al. (2024b)は推論能力を必要とするタスクの性能向上のためにKVキャッシュを拡張する手法を提案した。著者らは微分可能な「コプロセッサ」を用い、フォワードパス中に活性化を直接変更する代わりに、生成前のステップとしてKVキャッシュを拡張することを可能にしている。
しかしながら、KVキャッシュを拡張するためには、別途モデルを訓練する必要があり、
この手法は前のサブセクションで紹介した純粋な活性化ステアリング手法よりも実用性に劣る。
これに対し、我々のアプローチは補助モジュールを訓練することなく、
小規模モデルにおいて行動制御のターゲットとしてKVキャッシュを利用することを目指す。

## キャッシュステアリング
### 予備知識
トランスフォーマーベースの言語モデルは、クエリ、キー、値のベクトル集合に対して動作する自己注意機構に依存し、文脈化されたトークン表現を計算する。与えられた入力シーケンスに対して、層 l における注意出力は以下のように計算される：
Attention(Q^l, K^l,V^l) = softmax Q^l (K^l)^⊤ √Dh! V^l
ここで、Q^l, K^l,V^l ∈ R^{T×H×D_h}は層 l におけるクエリ、キー、値テンソルであり、T はシーケンス長、H はアテンションヘッド数、Dh は各ヘッドの次元数である。
自己回帰的デコード中、モデルは処理済みトークンに対応するキー K^l と値 V^l を保存する。これはキー・バリュー（KV）キャッシュとして知られる。これらのキャッシュされたテンソルは、シーケンス全体の表現を再計算することなく、各新規トークンに対する注意を効率的に計算するために使用される。重要な点として、これらのキャッシュエントリは事前計算され、複数の例（システムプロンプトのキャッシュなど）で再利用可能である。これは大規模モデルや類似入力に対する反復推論を伴うシナリオで特に有用である。これによりKVキャッシュは行動介入の潜在的な対象となり、実世界設定との互換性を提供する。

### KVステアリングベクトルの抽出
活性化ステアリングと同様に、対照的なプロンプトペアの集合 C ={(p^+_0, p^−_0),(p^+_1, p^−_1), ...,(p^+_N, p^−_N )} を構築し、キー値ステアリングベクトルを抽出する。望ましい挙動を示すプロンプトを正例、該当しないプロンプトを負例と呼ぶ。
推論誘導タスクにおける正例・負例の構築詳細については
セクション3.5で議論する。
対照的な例ペアごとに、順伝播を行い、指定されたトークン位置（通常は入力プロンプトの最終トークン）からキーと値のベクトルを抽出する。その後、差の平均法を用いてベクトルを集約する：
S^k_l =1/N \sum_{(p+,p−)∈C} f_l(p^+) − f_l (p^−)
S^v_l =1/N \sum_{(p+,p−)∈C} f_l(p^+) − f_l (p^−)
ここで、f_l はトランスフォーマー層であり、S^k_l ∈ R^{H×D_h} および S^v_l ∈ R^{H×Dh} は層 l における結果のステアリングテンソルであり、H はアテンションヘッドの数を、Dh はその次元を表す。正例と負例の差分を取り、複数の対比ペア間で平均化することで、個々の例から生じる情報のノイズを最小化しつつ、目標行動に関連する方向性信号を抽出することを目指す。

### KVステアリングベクトルの適用
推論時、入力プロンプトに対して標準的な順伝播を実行し、KVキャッシュを埋める。
その後、各層 l において、KVキャッシュ内のターゲットトークン位置に対応するキャッシュ済みキーベクトルと値ベクトルを以下のように修正する：
V^∗_l = V_l + c^v S^v_l
K^∗_l = K_l + c^k S^k_l
ここで、K_l, V_l ∈ R^{H×D_h} は層 l における元のキャッシュ済みキー・値ベクトルであり、S^k_l, S^v_l ∈R^{H×D_h} はステアリングベクトル、c^k, c^v ∈ R はステアリング強度を制御するスカラー係数である。
その後、修正されたキャッシュを用いて通常通り生成を継続する。

### 直観
キャッシュステアリングは、生成過程におけるモデルの挙動に影響を与える方法とタイミングにおいて、従来の活性化ステアリングと根本的に異なる。主な相違点は図1に視覚的に示されている。以下に、本手法の核心となる直観を概説する。

1. クエリの変更なし。
活性化ステアリングは通常、クエリ・キー・値を生成する隠れ状態を変更する。クエリの変更は入力条件付け能力を低下させる可能性がある。これに対し、キャッシュステアリングは過去のトークンから保存されたキーと値のみを変更し、クエリは変更しない。これによりモデルはデコード時に入力文脈のより正確な表現を維持できる。
2. 過去を制御する vs 現在を制御する。
特定の時間ステップ t において、活性化制御は明示的に層 l の現在の隠れ状態に影響を与え、それがその後、モデル内の層数 N までの全ての後続層 l+1 から l+N へと伝播する。これは「オーバーステアリング」として知られる、層間で小さな介入が複合的に作用する連鎖効果を引き起こす可能性があり、介入後の生成品質に悪影響を及ぼすことがある。この特性により、活性化ステアリングはステアリング強度や適用層といった特定のハイパーパラメータに極めて敏感であることが広く実証されている。対照的に、キャッシュステアリングは過去のトークンの固定されたキーと値の表現を修正する。これらの値は層を通ってさらに処理されるのではなく、将来のクエリによって直接アテンションされる。つまり、キャッシュステアリングは効果の集約なしに多くの（あるいは全ての）層に適用可能であり、将来の隠れ状態に暗黙的な影響を与える。
3. ソース行動との整合性。
重要な点として、我々のステアリングベクトルはCoT推論トレースを含むプロンプトのキーと値から抽出される。標準的な少例学習プロンプティングシナリオでは、これらのキー値ペアは過去のキャッシュに存在し、生成時にアテンションされる。推論時にKVキャッシュにステアリングベクトルを適用することで、例を明示的に含めることなくそのスタイルを転移できることを期待している。

## 実装の詳細
### 対照的セット構築
ステアリングベクトルを抽出するため、対照的データセットを構築する。これはペアになったプロンプトで構成される。各ペアには、肯定例（明示的な思考の連鎖を含む）と否定例（最終回答のみを含む）が含まれる。各対照プロンプトは、少例学習（ICL）例を用いて作成される。具体的には、肯定例と否定例の両方に、n個のICL例とそれに続く質問および生成プロンプトが含まれる。肯定例と否定例の違いは、ICL例に推論ステップが含まれるか否かのみである（詳細は付録Bおよび例を参照）。

### 抽出と適用位置
プロンプトの最終トークンからキーと値のベクトルを抽出する。これはモデルのチャットテンプレートに依存するが、通常は生成プロンプトの最終トークンに対応する（例：「assistant\n\n」内の「\n\n」）。推論時には、抽出時に使用したプロンプト内の同一論理位置にキャッシュステアリングを適用することを目指す。ただし、自己回帰的デコード機構（3.1節参照）により、KVキャッシュは各トークン処理後にのみ更新される。位置合わせを確保するため、プロンプトに中立オフセットトークン（改行や空白など）を追加する。これにより最終トークンのKV表現を次のトークン生成に利用可能となり、キャッシュステアリングが意図した位置に影響することを保証する。トークン位置合わせとキャッシュオフセットの詳細は付録Fに記載する。

### ハイパーパラメータ
活性化ステアリングと同様に、キーと値のベクトルのステアリング強度係数は
ハイパーパラメータとして扱われます。より大きなモデルから推論行動を蒸留することに関心があるため、
対照ペアの数と各ペアの文脈内例の数も追加のハイパーパラメータとして扱います。
他のステアリング手法と同様に、 他のステアリング手法と同様に、
各モデルとデータセットのペアについて、ハイパーパラメータに対して小規模なグリッド検索を行い、妥当な値を取得します。ステアリング係数は、タスク間で一貫した範囲内に収まる傾向があり、この手法の挙動の頑健性を示唆していることがわかりました。
これについては、セクション 5.2 で詳しく説明します。ハイパーパラメータの完全なリストは、
付録 G に記載されています。

## 実験の準備
### データセット
評価には4つの一般的な推論ベンチマークを使用する：GSM8K（Cobbe et al., 2021）、CommonsenseQA（Talmor et al., 2018）、ARC-Challenge（Clark et al., 2018）、PIQA（Bisk et al., 2020）。これらのデータセットは、算術推論、常識推論、科学的な質問、物理的な常識推論を網羅している。各データセットについて、対応するトレーニングセットからの質問のサブセットに対して、GPT-4oを用いて精巧な段階的な回答を生成し、対照セットの肯定例として使用する。これらのステップ生成に使用した具体的なプロンプトと生成手順の詳細は付録Jに記載されている。ステアリングベクトルはトレーニングセットを用いて計算され、評価は対応するテストセットで実施される。

### モデル
我々は、4つのファミリーに属する小規模な命令特化モデル（Llama3.2（1Bおよび3Bバリアント）、SmolLM2（360M）、Qwen2（0.5B）、Phi-4-mini（3.8B））におけるキャッシュステアリングを評価する（Grattafiori et al., 2024; Team, 2024; Allal et al., 2025; Abouelenin et al., 2025）。さらに、最小モデルを超えるスケールでのキャッシュステアリング評価のため、Llama-3.1（8B）モデルを追加した。完全なモデル名とURLの一覧は付録Hに記載されている。

### デコード戦略
キャッシュステアリングは内部表現に影響を与え、出力ロジットのシフトをもたらすため、我々のアプローチを決定論的デコードと確率的デコードの両方で評価する。サンプリングベースのデコードでは、5つの異なるシードで応答を生成し、同じ設定を用いたベースライン生成と比較することで、ステアリング効果の一貫性を評価する。生成に関する議論は付録Eに記載されている。

### 回答抽出と評価指標
回答の正誤判定にはタスク固有のヒューリスティックを用いる。
GSM8Kでは、出力文中の最終数字を数字パターンマッチングにより抽出する（Wang et al., 2023; Wang and Zhou, 2024）。多肢選択問題（ARC, PIQA, CSQA）では、既知の選択肢に対するソフト文字列マッチングを基盤とし、制約付きデコーディングへのフェイルオーバーを備えた4段階抽出パイプラインを開発した。解答抽出プロセスの詳細は付録Cを参照のこと。

### 活性化ステアリングとの比較
複数実験において、キャッシュステアリングと活性化ステアリングを比較した。具体的には、活性化ステアリングで最も普及している手法の一つであるCAA法（Panickssery et al., 2023）を採用した。全ての実験において、公平な比較を実現するため最大限の努力を払っている。活性化ステアリングベクトルの抽出・適用方法の詳細は付録Dに記載する。

## 実験
キャッシュステアリングが推論行動を誘導する効果を評価するため、標準的な貪欲なデコード（介入なし）、CoTプロンプティング（「段階的に考えましょう」をプロンプトに追加）、活性化ステアリングといった複数のベースラインと比較した。さらに、CoTプロンプティングとキャッシュステアリングを組み合わせたハイブリッド手法も評価した。表1（貪欲部分）に示す通り、キャッシュステアリングは一貫してベースラインを上回り、CoTプロンプティングよりも高い性能を発揮するケースが多い。
さらに、CoTプロンプティングとキャッシュステアリングの組み合わせは、半数以上のケースで追加的な性能向上をもたらし、両手法の相補性を示している。特筆すべきは、キャッシュステアリングがほぼ全てのケースで活性化ステアリングを上回った点である。

さらに、全データセットの平均値として、モデルごとに生成されたトークンの平均数を表2に報告する（各データセットペアの個別結果は付録Oに記載）。キャッシュステアリングはより長い出力を生み出し、CoTプロンプトによる補完すら上回る。これは、明示的なプロンプトがなくても、この介入がより精緻な推論を促すことを示唆している。両表の結果を考慮すると、キャッシュステアリングがより精緻な推論をもたらすと結論付けられる。重要な点として、キャッシュステアリングが推論誘導に有効であるにもかかわらず、それだけではベンチマークでの精度向上を保証しない。場合によっては、構造化され妥当な推論トレースを生成しながらも、誤った結論に至る（例：質問の曖昧性や代替的な有効解釈による）ことがある。また、複雑な推論を選択ラベルに結び付けられないモデルの不具合（つまり、正解を生成するものの誤ったラベルを付与する）が、正しい推論トレースを誤りと判定する原因となる場合があることも観察された。読者はコードリポジトリ内の定性的出力を検証することを推奨する。そこでは、応答の構造や様式にステアリング効果が確認できる。付録Aにもいくつかの定性的例を掲載している。

#### 定性的検証
キャッシュステアリングが実際に推論行動を誘導することを確認するため、付録Aに代表的な完成例を複数提示する。ほぼ全てのケースにおいて、キャッシュステアリングは、ベースラインの出力が浅い、あるいは突然終了する場合であっても、モデルに多段階の論理的に構造化された回答を生成させる。

#### サンプリング下での安定性
表1の右側は、サンプリングベースのデコーディング下での結果を示し、複数のモデルとタスクにわたってキャッシュステアリングとベースラインを比較している。キャッシュステアリングは一貫した性能向上をもたらすか、ベースラインと同等の性能を維持していることが確認できる。これは本手法がモデルのロジット値に安定かつ意味のある変化をもたらすことを示唆している。ノイズの注入や不安定な挙動の誘発ではなく、キャッシュステアリングは確率的生成下においても、体系的にモデルをより構造化された推論へと誘導する。実行間の標準偏差が比較的小さいことも、この効果の頑健性をさらに裏付けている。

### 解析研究
Llama-3.2-1B-InstructおよびARC-cデータセットを用いて解析実験を実施し、キャッシュステアリングの主要ハイパーパラメータに対する感度を評価した：
1) ステアリングベクトル抽出に使用する対照ペアの数
2) 対照例ごとの少例学習例の数
3) ステアリング強度係数 c^k および c^v
全解析研究の結果を図2に示す。

#### ベクトル抽出
対比ペア数を100から1000まで変化させた。この範囲では精度が比較的安定し、わずかな変動（53.1%～55.7%）のみが確認された。これは、対比セットが小さくても効果的なステアリングベクトルを生成できることを示唆するが、セットが大きくなるほど性能はわずかに向上する傾向がある。また、プロンプトごとのICL例数を1から10まで変化させた。興味深いことに、単一例（55.8%）で最高結果が得られ、3ショット（52.4%）でわずかに低下した後回復する。この非単調な傾向は、推論シグナルが訓練データ内の特定例に敏感であることを示唆している。

#### ベクトル適用。
より重要なのは、キャッシュステアリングがステアリング強度の変動に対して頑健である点だ。キー係数c^kを0.0から0.4まで変化させてもわずかな変化しか生じず、c^k = 0.3（56.4%）で最高の性能を示す。値係数 c^v を 1 から 10 まで変化させると、c^v = 6 付近でピーク（55.0%）を示し、その後徐々に低下し、c^v = 8 を超えるとパフォーマンスは 52% 以下に低下します。極端なハイパーパラメータはパフォーマンスのわずかな低下につながる場合がありますが、キャッシュステアリングは係数の局所的な変化に対して安定性を維持します。対照的に、アクティベーション・ステアリングは、係数値のわずかな変化が壊滅的な生成の失敗につながるなど、高い感度を示すことが多い（Panickssery et al., 2023; Turner et al., 2024; Da Silva et al., 2025）。付録 N では、ARC-c のより小さなサブセットを用いて、アクティベーション・ステアリングのハイパーパラメータに対する感度を示しています。

#### 計算オーバーヘッド
キャッシュステアリングは生成前に一度だけキャッシュ変更を行うだけで済むのに対し、アクティベーションステアリングは効果を発揮するために継続的な介入が必要な場合がほとんどである（Wehner et al., 2025）。図3に示す通り、キャッシュステアリングはベースライン（介入なし）と同等のレイテンシを達成する一方、アクティベーションステアリングは特に大きなバッチサイズにおいてトークンあたりの時間が大幅に増加する。これらの知見はキャッシュステアリングの実用的な効率性を裏付け、実世界の展開シナリオに極めて適していることを示している。実験の詳細は付録Mに記載されている。

### スタイル転送
キャッシュステアリングを用いて教師モデルから異なる推論スタイルを抽出できるかを検証するため、ベクトル抽出に用いる推論トレースの様式が応答構造に与える影響を評価する。本実験ではARC-Challengeデータセットから20問を抽出し、各問題に正しい多肢選択式解答を対応させた。各問題に対し、同一解答に至るが構造が異なる6種類の推論トレースを構築した。6つのスタイルの説明は表3に示す。
各スタイルごとに1つのステアリングベクトルを抽出する。推論時、ARC-Challengeデータセットのテストセットから抽出した20問の質問群に対し、各スタイル固有のステアリングベクトルを適用し、結果の出力構造への影響を検証する。本実験では、サイズが小さいことからSmolLM-360M-Instructモデルを使用する。

表4は、各スタイル固有のステアリングベクトルに対して生成された応答のうち、意図した構造に一致する割合を示している。結果は、段階的推論、因果連鎖、類推的推論の各スタイルにおいて、キャッシュステアリングがほぼ全てのテストケースで正しい構造を誘導することに成功したことを示している。これらのケースでは、出力はステアリングベクトル抽出に使用したテストデータのパターンに一貫して準拠しており、スタイル信号がキーバリューキャッシュ介入を通じて堅牢に捕捉・伝達されていることを示唆している。
対照的に、スタイル転送は「戦略＋実行」形式と「注釈付き演繹」形式では信頼性が低い。生成文の半数のみが戦略実行構造を反映し、注釈付き演繹スタイルに一致するのは20件中わずか3件である。これらの失敗要因を理解するため、出力の定性分析を実施した。注釈付き演繹形式の場合、この形式がモデルの事前学習分布において過小評価されていると仮説を立てた。生成文の大半は、角括弧で囲まれた語句や句で始まる（例：[ワイオミング州の農場は...]）といった部分的な様式的アーティファクトを示すものの、ベクトル抽出時に使用された陽性例に見られる構造化された論理的展開を欠いている。多くの場合、ステアリング信号は望ましい文体の方向へモデルを「誘導」しているように見えるが、完全な順守を引き出すには不十分か、あるいは意味のあるテキストを生成するには強すぎるかのいずれかである。
戦略＋実行形式でも同様のパターンが確認された。全ての応答は正しい談話マーカー（例：Strategy:）で始まるものの、生成サンプルの半数は同じマーカーをループで繰り返す（例：Strategy:, Strategy:, ...）。この不具合は過度のステアリングが原因と考えられる。各スタイルごとにステアリング係数を明示的に調整していないため、デフォルト値がこの特定のケースでは高すぎる可能性があり、退化した出力を引き起こしている。
図4は、5つのステアリングスタイルそれぞれで単一のARC-Challenge問題に対して生成された応答の冒頭部分を定性的に示した例である。これらは、生成物間の修辞的差異が検出可能であるだけでなく、しばしば顕著であることを示している。例えば、類推的ステアリングベクトルで生成された回答は全て「ちょうど～のように…」で始まり、因果連鎖の例は条件付き論理パターンに従っている。これらの観察結果は、文体信号が実際にKVキャッシュにエンコードされており、キャッシュステアリングを用いて任意のプロンプトに継承できることを裏付けている。

これらの結果を総合すると、キャッシュステアリングは一般的な推論の誘導だけでなく、その形式をより細かく制御するために活用できることが示唆される。これにより、モデルの解釈可能性、ユーザーとの整合性、説明の質を向上させるためのスタイル選択など、制御可能な推論戦略に関する今後の研究の可能性が開かれる。

## 制限事項
他の手法と同様に、キャッシュステアリングにも制限がある。本研究では、小規模な大規模言語モデル（LLM）における推論行動の誘導に焦点を当てている。結果は有望ではあるものの、キャッシュステアリングがより大規模なモデルや異なる領域、あるいは推論以外のタスク（指示順守や安全性の整合性など）にどの程度一般化できるかは今後の課題である。これらのより広範なユースケースへのキャッシュステアリングの拡張には、さらなる実証的・理論的研究が必要である。

## 結論
我々は、キーバリュー（KV）キャッシュへのワンショット修正を通じて言語モデルの推論行動を誘導する新規技術「キャッシュステアリング」を導入した。対照例とGPT-4oが生成したCoT推論トレースを活用することで、本手法は微調整やプロンプト設計、生成時の継続的介入を必要とせず、小型モデルにおいて構造化された推論を誘導する。
従来の活性化ステアリングとは異なり、キャッシュステアリングは過去のトークンの保存済み表現のみを修正するため、安定性の向上、標準推論パイプラインとの互換性、推論時の効率化を実現する。GSM8K、ARC-Challenge、CSQA、PIQAでの実験を通じて、
キャッシュステアリングが推論行動を確実に誘導し、場合によってはタスク精度を向上させ得ることを実証した。推論誘導に加え、キャッシュステアリングが推論スタイルの制御を可能にすることを示す。キャッシュステアリングは完全なプラグアンドプレイではないものの、その有効性は係数とステアリングベクトルの選択に依存する。言語モデルにおける行動制御のための有望かつ軽量なメカニズムを提供する。本研究が、制御可能な生成、推論スタイル転移、キーバリュー空間における低コストの知識蒸留技術に向けた新たな方向性を切り開くことを期待する。

---
DeepL.com（無料版）で翻訳しました。