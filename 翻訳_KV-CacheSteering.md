## 要約
我々は、キーバリューキャッシュに直接適用されるワンショット介入による言語モデルの暗黙的誘導を実現する軽量手法「キャッシュステアリング」を提案する。その有効性を検証するため、小規模言語モデルにおいて思考連鎖推論を誘導するためにキャッシュステアリングを適用した。本手法はGPT-4oが生成した推論トレースを活用し、微調整やプロンプト変更なしにモデルの挙動をより明示的で多段階の推論へと導く誘導ベクトルを構築する。多様な推論ベンチマークにおける実験評価により、キャッシュステアリングが
モデル推論の質的構造と定量的タスク性能の両方を向上させることが実証された。
継続的な介入を必要とする従来の活性化ステアリング技術と比較して、
我々のワンショットキャッシュステアリングは、ハイパーパラメータの安定性、
推論時間の効率性、統合の容易性の面で大きな利点を提供し、
制御された生成のためのより堅牢で実用的な解決策となる。

## はじめに
大規模言語モデルが複雑な推論を実行する能力は、その有用性が高まる主要な要因である。しかし、この潜在能力は常に自発的に発揮されるわけではなく、特に潜在的な推論能力を有しながらも特定の誘導を必要とする小規模モデルでは顕在化しない。教師あり微調整や思考連鎖例を用いた少例学習プロンプティングといった従来の能力発掘手法は有効だが、膨大なデータや複雑なプロンプト設計を要することが多い。そこで疑問が生じる：トレーニング後にこれらの固有推論プロセスを解き放ち誘導する、より軽量な介入手法を開発できるだろうか？

有望な方向性の一つが活性化誘導（Turner et al., 2024; Panickssery et al., 2023）である。これはモデルの内部隠れ状態を直接変更することで、その挙動を導くことを目的とする。再学習なしで出力に影響を与え得る点で有望である一方、活性化誘導は効果を発揮するためにデコードプロセス全体を通じて各トークン生成ステップで継続的な介入を必要とする場合が多い（Wehner et al., 2025）。この継続的な操作は不安定性を生じさせ、結果を超パラメータの選択（対象とする層、介入強度など）に極めて敏感にし、生成品質の低下を招く可能性がある。

これらの課題に対処するため、我々は「キャッシュステアリング」と呼ばれる手法を導入する。本アプローチは、Transformerモデルのキーバリューキャッシュに対して直接、標的を絞った単発の修正を行うことで動作する。
通常、キャッシュは最初のプロンプトによって初期化されます。GPT-4oのような高性能な教師モデルが生成した推論トレースから導出されたステアリングベクトルを、これらのキャッシュされたキーと値の表現に適用することで、小型モデルの推論軌道を誘導できます。この単一の介入は、トークン生成開始前に適用されるため、モデル重みを変更したり複雑なプロンプト修正を必要とせずに、より明示的で多段階の推論へと効果的に誘導します。

これにより以下の主要な利点が得られる：
ハイパーパラメータ変動に対する安定性と頑健性の向上、
継続的な介入が不要なため推論時の計算オーバーヘッドの大幅削減、
標準的なトランスフォーマー推論パイプラインとのシームレスな統合。
我々の手法は推論構造を改善し、GSM8K、ARC-Challenge、CSQA、PIQAを含む
複数のベンチマークにおいて多くのケースでタスク精度を向上させることを実証する。
全体として、我々の主な貢献は以下の通りである：
• キャッシュステアリングを提案する。これは推論時にKVキャッシュをワンショットで変更することでモデル挙動を制御する新規技術である。
• キャッシュステアリングにより、GPT-4oなどの大規模モデルから推論スタイルを蒸留し、
微調整やプロンプト拡張なしに小型モデルへ転移できることを実証。
• 推論ベンチマークで広範な評価を実施し、活性化ステアリングやCoTプロンプティングと比較。
効率性と安定性を分析。

## 関連研究
### 推論と思考の連鎖プロンプティング。
LLMの推論能力を強化する広く採用されている手法には、言語モデルへのプロンプト内で段階的な推論プロセス（思考の連鎖：CoT）を含む問題の解決例を示す方法（文脈内学習：ICL）がある（Brown, 2020; Wei et al., 2022）。この手法は「少例学習プロンプティング」として知られる。
CoTプロンプティングのゼロショット変種は、「段階的に考えよう」といった指示を追加することで、
例示の提示を必要とせずに段階的推論を誘導し、このアプローチを簡略化する（Kojima et al., 2022）。
最近の研究では、強化学習が顕著な推論能力をもたらし、
教師あり微調整を通じて小型モデルへ効果的に蒸留できることが示されている（Guo et al., 2025）。
これらの知見は、言語モデルでCoT推論を単に誘発するだけでは不十分であり、推論の様式が重要であることを示唆している。これが我々のアプローチの動機であり、キャッシュレベルでの介入を通じて、大規模な教師モデルを彷彿とさせる推論行動へ小型モデルを直接誘導することを目指す。

### 活性化ステアリング。
活性化ステアリング（別名：表現工学）は、通常は線形介入を通じて、デコード中の中間活性化を操作することでLLMの生成プロセスを暗黙的に制御する技術である（Panickssery et al., 2023; Turner et al., 2024）。複数の研究が、再学習なしにモデルに特定の行動を誘導または抑制するために活性化ステアリングを適用している。例としては、感情・トピック・スタイル制御（Turner et al., 2024）、機能ステアリング（Todd et al., 2023; Postmus and Abreu, 2024）、拒否行動の除去または誘導、 （Lee et al., 2024）、有害性の低減（Turner et al., 2024）、真実性（Wang et al., 2025）、
事実知識の編集（Yin et al., 2024）、推論（Zhang and Viteri, 2025; Galichin et al., 2025)
その他（Wehner et al., 2025）などである。
最も基本的な形態において、活性化ステアリングは2つのステップを含む：ベクトル抽出と推論時のモデル活性化へのベクトル注入。ベクトル抽出段階では「ステアリングベクトル」を計算する。これは通常、望ましい挙動を持つ肯定的プロンプトと否定的あるいは中立的なプロンプトのペアから活性化を集約することで行われ、対照セットt$C = {(p^+_0, p^−_0),(p^+_1, p^−_1), ...,(p^+_N , p^−_N)}$を形成する。最も一般的な集約手法はDifferencein-Means（Wehner et al., 2025）であり、ベクトルがペア化されている場合にはMean-of-Differencesと同一となる：
$$s_l = \frac{1}{N} \sum_{(p^+,p^−)∈C} f_l(p^+) − f_l(p^−)$$

ここで、fl は層 l における Transformer モデルの一部（例：デコーダ層全体）を表し、
N は対照データセット C 内の例の数である。モデルの出力の方向を制御するため、推論中に特定の層の活性化値にステアリングベクトルを加算する：
$$h^∗_l = h_l + c s_l$$
ここで hl はステアリング前の層 l における活性化値、sl は層 l から抽出されたステアリングベクトル、c はステアリングの強さを決定する係数である。
重要な点として、このベクトルは異なるトークン位置、層、モデル部分に対して抽出・適用可能であり、これらはハイパーパラメータまたは設計上の選択として扱われる。通常、ステアリングを適用する層とステアリング強度係数 c の値を決定するにはグリッド検索を行うのが一般的である（Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024）。
活性化ステアリングはモデル制御の手段を提供する一方で、生成中に継続的な介入を必要とするのが一般的である（Wehner et al., 2025）。これはコストが高く、不安定な生成を引き起こす可能性がある。これに対し、我々の研究は活性化ステアリングの原理をキーバリュー（KV）キャッシュに拡張し、推論時に効率的かつ安定性の高いワンショット介入を可能にする。

### キャッシュ操作。
別の新たな研究分野では、メモリと効率性の観点からキーバリュー（KV）キャッシュを改変するアイデアが探求されている（Li et al., 2024; Liu et al., 2024a; Ge et al., 2023; Mu et al., 2023）。これらのアプローチは、KVキャッシュ操作を通じてメモリフットプリントの削減や文脈表現の圧縮を目指す。この概念を発展させ、Liu et al. (2024b)は推論能力を必要とするタスクの性能向上のためにKVキャッシュを拡張する手法を提案した。著者らは微分可能な「コプロセッサ」を用い、フォワードパス中に活性化を直接変更する代わりに、生成前のステップとしてKVキャッシュを拡張することを可能にしている。
しかしながら、KVキャッシュを拡張するためには、別途モデルを訓練する必要があり、
この手法は前のサブセクションで紹介した純粋な活性化ステアリング手法よりも実用性に劣る。
これに対し、我々のアプローチは補助モジュールを訓練することなく、
小規模モデルにおいて行動制御のターゲットとしてKVキャッシュを利用することを目指す。

### 予備知識
トランスフォーマーベースの言語モデルは、クエリ、キー、値のベクトル集合に対して動作する自己注意機構に依存し、文脈化されたトークン表現を計算する。与えられた入力シーケンスに対して、層 l における注意出力は以下のように計算される：
Attention(Q^l, K^l,V^l) = softmax Q^l (K^l)^⊤ √Dh! V^l
ここで、Q^l, K^l,V^l ∈ R^{T×H×D_h}は層 l におけるクエリ、キー、値テンソルであり、T はシーケンス長、H はアテンションヘッド数、Dh は各ヘッドの次元数である。
自己回帰的デコード中、モデルは処理済みトークンに対応するキー K^l と値 V^l を保存する。これはキー・バリュー（KV）キャッシュとして知られる。これらのキャッシュされたテンソルは、シーケンス全体の表現を再計算することなく、各新規トークンに対する注意を効率的に計算するために使用される。重要な点として、これらのキャッシュエントリは事前計算され、複数の例（システムプロンプトのキャッシュなど）で再利用可能である。これは大規模モデルや類似入力に対する反復推論を伴うシナリオで特に有用である。これによりKVキャッシュは行動介入の潜在的な対象となり、実世界設定との互換性を提供する。

---
DeepL.com（無料版）で翻訳しました。