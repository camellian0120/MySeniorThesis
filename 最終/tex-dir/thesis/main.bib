@misc{transformers,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{codebert,
      title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
      author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
      editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
      booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
      month = nov,
      year = "2020",
      address = "Online",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2020.findings-emnlp.139/",
      doi = "10.18653/v1/2020.findings-emnlp.139",
      pages = "1536--1547",
      abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both ``bimodal'' data of NL-PL pairs and ``unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing."
}

@misc{graphcodebert,
      title={GraphCodeBERT: Pre-training Code Representations with Data Flow}, 
      author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
      year={2021},
      eprint={2009.08366},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2009.08366}, 
}

@misc{llminsoftwaresecurity,
      title={LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights}, 
      author={Ze Sheng and Zhicheng Chen and Shuning Gu and Heqing Huang and Guofei Gu and Jeff Huang},
      year={2025},
      eprint={2502.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2502.07049}, 
}

@misc{rag,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@techreport{finetuning,
      title={Improving Language Understanding by Generative Pre-Training},
      author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
      institution={OpenAI},
      year={2018},
      url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@misc{hiddenriskllmgeneratedweb,
      title={The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models}, 
      author={Swaroop Dora and Deven Lunkad and Naziya Aslam and S. Venkatesan and Sandeep Kumar Shukla},
      year={2025},
      eprint={2504.20612},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.20612}, 
}

@misc{llmcyber,
      title={Large Language Models for Cyber Security: A Systematic Literature Review}, 
      author={Hanxiang Xu and Shenao Wang and Ningke Li and Kailong Wang and Yanjie Zhao and Kai Chen and Ting Yu and Yang Liu and Haoyu Wang},
      year={2025},
      eprint={2405.04760},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2405.04760}, 
}

@misc{devigneffectivesearchvulnerability,
      title={Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks}, 
      author={Yaqin Zhou and Shangqing Liu and Jingkai Siow and Xiaoning Du and Yang Liu},
      year={2019},
      eprint={1909.03496},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/1909.03496}, 
}

@article{chess_mcgraw,
      title={Static Analysis for Security},
      author={Chess, Brian and McGraw, Gary},
      journal={IEEE Security \& Privacy},
      year={2004}
}

@article{hinton2006reducing,
      title   = {Reducing the Dimensionality of Data with Neural Networks},
      author  = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
      journal = {Science},
      volume  = {313},
      number  = {5786},
      pages   = {504--507},
      year    = {2006},
      publisher = {American Association for the Advancement of Science},
      doi     = {10.1126/science.1127647}
}

@inproceedings{uesaka2017multi_view,
      title     = {Multi-view Learning over Retinal Thickness and Visual Sensitivity on Glaucomatous Eyes},
      author    = {Uesaka, Toshimitsu and Morino, Kai and Sugiura, Hiroki and Kiwaki, Taichi and Murata, Hiroshi and Asaoka, Ryo and Yamanishi, Kenji},
      booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
      series    = {KDD '17},
      pages     = {2041--2050},
      year      = {2017},
      publisher = {Association for Computing Machinery},
      doi       = {10.1145/3097983.3098194}
}

@misc{reimers2019sentencebert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1908.10084}, 
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04906}, 
}

@misc{e5,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.03533}, 
}

@article{cve-llm,
      title   = {CVE-LLM: Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models},
      author  = {Zhang, Yifan and others},
      journal = {arXiv preprint},
      year    = {2023}
}

@misc{IPA_safe_web,
      author       = {{情報処理推進機構（IPA）}},
      title        = {安全なウェブサイトの作り方 改訂第7版},
      howpublished = {\url{https://www.ipa.go.jp/security/vuln/websecurity.html}},
      year         = {2023},
      note         = {参照日: 2026-02-05}
}

@misc{chen2021evaluatinglargelanguagemodels,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@misc{zhou2024largelanguagemodelvulnerability,
      title={Large Language Model for Vulnerability Detection and Repair: Literature Review and the Road Ahead}, 
      author={Xin Zhou and Sicong Cao and Xiaobing Sun and David Lo},
      year={2024},
      eprint={2404.02525},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.02525}, 
}

@misc{IPA_top10_2025,
      author       = {{情報処理推進機構（IPA）}},
      title        = {情報セキュリティ10大脅威 2025},
      howpublished = {\url{https://www.ipa.go.jp/security/10threats/10threats2025.html}},
      year         = {2025},
      note         = {参照日: 2026-02-05}
}

@article{bessey_sast,
      author  = {Bessey, Al and others},
      title   = {A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World},
      journal = {Communications of the ACM},
      volume  = {53},
      number  = {2},
      pages   = {66--75},
      year    = {2010}
}

@inproceedings{miller_fuzzing,
      author    = {Miller, Barton P. and others},
      title     = {An Empirical Study of the Reliability of UNIX Utilities},
      booktitle = {Communications of the ACM},
      year      = {1990}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{russakovsky2015imagenetlargescalevisual,
      title={ImageNet Large Scale Visual Recognition Challenge}, 
      author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
      year={2015},
      eprint={1409.0575},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.0575}, 
}
