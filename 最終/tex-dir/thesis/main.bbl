\begin{thebibliography}{6}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu,
  Jiang, and Zhou]{codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
  Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
\newblock {C}ode{BERT}: A pre-trained model for programming and natural
  languages.
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Findings of
  the Association for Computational Linguistics: EMNLP 2020}, pages 1536--1547,
  Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.139}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.139/}.

\bibitem[Guo et~al.(2021)Guo, Ren, Lu, Feng, Tang, Liu, Zhou, Duan,
  Svyatkovskiy, Fu, Tufano, Deng, Clement, Drain, Sundaresan, Yin, Jiang, and
  Zhou]{graphcodebert}
Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou,
  Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao~Kun Deng,
  Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming
  Zhou.
\newblock Graphcodebert: Pre-training code representations with data flow,
  2021.
\newblock URL \url{https://arxiv.org/abs/2009.08366}.

\bibitem[Lewis et~al.(2021)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K^^c3^^bcttler, Lewis, tau Yih, Rockt^^c3^^a4schel, Riedel, and Kiela]{rag}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K^^c3^^bcttler, Mike Lewis, Wen tau Yih, Tim
  Rockt^^c3^^a4schel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks,
  2021.
\newblock URL \url{https://arxiv.org/abs/2005.11401}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{finetuning}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock Technical report, OpenAI, 2018.
\newblock URL
  \url{https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}.

\bibitem[Sheng et~al.(2025)Sheng, Chen, Gu, Huang, Gu, and
  Huang]{llminsoftwaresecurity}
Ze~Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, and Jeff Huang.
\newblock Llms in software security: A survey of vulnerability detection
  techniques and insights, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.07049}.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\end{thebibliography}
