# Task
1. 1ãƒšãƒ¼ã‚¸ã‹ã‚‰ç›®çš„ã®æƒ…å ±ã‚’æŠœãå‡ºã™ [wip]
2. å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰ç›®çš„ã®æƒ…å ±ã‚’æŠœãå‡ºã™ [x]

---
<br/>

# query-task2
ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- scrapy

## å®Ÿè£…ä»•æ§˜
- scrapyã§ä»¥ä¸‹ã®WEBã‚µã‚¤ãƒˆã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°\
"https://jvndb.jvn.jp/ja/contents/2025/JVNDB-2025-016156.html"
- ä¸Šè¨˜ã‚µã‚¤ãƒˆå†…ã«å«ã¾ã‚Œã‚‹"https://www.cve.org/CVERecord?id=CVE-XXXX-XXXXXX.html"å†…ã‚‚ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°
- scrapyã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯"jvn_spider"ã¨ã—ã€ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã¯"JvnSpider"ã¨ã™ã‚‹
- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å†…å®¹ã¯ã€ä»¥ä¸‹ã®5ã¤
1. "JVNDB-"ã‹ã‚‰å§‹ã¾ã‚‹è„†å¼±æ€§ç•ªå·
2. "è„†å¼±æ€§"ã§çµ‚ã‚ã‚‹è„†å¼±æ€§ã®æ¦‚è¦
3. JVNDBã®ã‚µã‚¤ãƒˆå†…ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹å½±éŸ¿ã‚’å—ã‘ã‚‹æŠ€è¡“
4. JVNDBã®ã‚µã‚¤ãƒˆå†…ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹å½±éŸ¿ã‚’å—ã‘ã‚‹æŠ€è¡“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³
5. CVERecordã®ã‚µã‚¤ãƒˆå†…ã®Description
- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã¯jsonã«ä¿å­˜
- jsonãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã¯ã€ä¸Šè¨˜ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸå†…å®¹ã‚’å«ã¿ã€ä»¥ä¸‹ã®ä¾‹ã«å¾“ã†

## å‡ºåŠ›ã™ã‚‹.jsonãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹
```json
{
    "JVNDB-XXXX-XXXXX":{
        "title": "XXXã®è„†å¼±æ€§",
        "description": "An issue in pytorch v2.7.0 can lead to a Denial of Service (DoS) when a PyTorch model consists of torch.Tensor.to_sparse() and torch.Tensor.to_dense() and is compiled by Inductor.",
        "technologies": "pytorch",
        "version": "2.7.0 ãŠã‚ˆã³ãã‚Œä»¥å‰"
    },
}
```

## æ³¨æ„ç‚¹
- ã‚³ãƒ¼ãƒ‰ã¯èª­ã¿ã‚„ã™ãã‚·ãƒ³ãƒ—ãƒ«ã«è¨˜è¿°ã™ã‚‹
- ä¾‹å¤–å‡¦ç†(ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)ã‚’é©åˆ‡ã«è¿½åŠ ã™ã‚‹
- æ—¥æœ¬èªã§ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã¤ã‘ã‚‹

---
<br/>

# output2
ä»¥ä¸‹ã¯ã€ã‚ãªãŸã®è¦ä»¶ã‚’ã™ã¹ã¦æº€ãŸã™ `Scrapy` ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã™ã€‚

---

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆï¼ˆ`jvn_spider`ï¼‰

```
jvn_spider/
â”œâ”€â”€ jvn_spider/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ middlewares.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ jvn_spider.py  â† ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ­ãƒ¼ãƒ©
â”œâ”€â”€ scrapy.cfg
```

---

## âœ… å®Ÿè£…æ‰‹é †

### 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ

```bash
scrapy startproject jvn_spider
```

---

### 2. `items.py`ï¼šå‡ºåŠ›ç”¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’å®šç¾©

```python
# jvn_spider/items.py

import scrapy

class JvnItem(scrapy.Item):
    jvndb_id = scrapy.Field()
    title = scrapy.Field()
    description = scrapy.Field()
    technologies = scrapy.Field()
    version = scrapy.Field()
```

---

### 3. `jvn_spider.py`ï¼šã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†æœ¬ä½“

```python
# jvn_spider/spiders/jvn_spider.py

import scrapy
from jvn_spider.items import JvnItem

class JvnSpider(scrapy.Spider):
    name = "jvn_spider"
    allowed_domains = ["jvndb.jvn.jp", "cve.org"]
    start_urls = [
        "https://jvndb.jvn.jp/ja/contents/2025/JVNDB-2025-016156.html"
    ]

    def parse(self, response):
        item = JvnItem()

        try:
            # 1. JVNDB ID ã‚’æŠ½å‡º
            jvndb_id = response.css('h1::text').re_first(r"(JVNDB-\d{4}-\d+)")
            item['jvndb_id'] = jvndb_id

            # 2. ã‚¿ã‚¤ãƒˆãƒ« (æ¦‚è¦) ã‚’æŠ½å‡º
            title = response.css("div.vulnOverview h2::text").get()
            item['title'] = title.strip() if title else ""

            # 3. å½±éŸ¿ã‚’å—ã‘ã‚‹æŠ€è¡“
            tech = response.xpath("//th[contains(text(), 'å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚·ã‚¹ãƒ†ãƒ ')]/following-sibling::td//text()").get()
            item['technologies'] = tech.strip() if tech else ""

            # 4. ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
            version = response.xpath("//th[contains(text(), 'ãƒãƒ¼ã‚¸ãƒ§ãƒ³')]/following-sibling::td//text()").get()
            item['version'] = version.strip() if version else ""

            # 5. CVE Record ã®URLã‚’æŠ½å‡º
            cve_url = response.css("a::attr(href)").re_first(r"https://www\.cve\.org/CVERecord\?id=CVE-\d{4}-\d+")
            if cve_url:
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¦ Description ã‚’å–å¾—
                request = scrapy.Request(cve_url, callback=self.parse_cve)
                request.meta['item'] = item
                yield request
            else:
                self.logger.warning("CVE URL ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                item['description'] = ""
                yield item

        except Exception as e:
            self.logger.error(f"è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    def parse_cve(self, response):
        item = response.meta['item']
        try:
            # 5. CVE ã® Description ã‚’æŠ½å‡º
            desc = response.xpath("//h2[text()='Description']/following-sibling::p/text()").get()
            item['description'] = desc.strip() if desc else ""
        except Exception as e:
            self.logger.error(f"CVEãƒšãƒ¼ã‚¸è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            item['description'] = ""
        yield item
```

---

### 4. `pipelines.py`ï¼šçµæœã‚’ JSON å½¢å¼ã«å¤‰æ›ã—ã¦ä¿å­˜

```python
# jvn_spider/pipelines.py

import json
from collections import defaultdict

class JvnSpiderPipeline:
    def __init__(self):
        self.data = defaultdict(dict)

    def process_item(self, item, spider):
        self.data[item['jvndb_id']] = {
            "title": item['title'],
            "description": item['description'],
            "technologies": item['technologies'],
            "version": item['version']
        }
        return item

    def close_spider(self, spider):
        # JSON ãƒ•ã‚¡ã‚¤ãƒ«ã¸å‡ºåŠ›
        with open('jvn_results.json', 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=4)
```

---

### 5. `settings.py`ï¼šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æœ‰åŠ¹åŒ–

```python
# jvn_spider/settings.py

# å‡ºåŠ›ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰è¨­å®š
FEED_EXPORT_ENCODING = 'utf-8'

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ‰åŠ¹åŒ–
ITEM_PIPELINES = {
   'jvn_spider.pipelines.JvnSpiderPipeline': 300,
}
```

---

### âœ… å®Ÿè¡Œæ–¹æ³•

```bash
cd jvn_spider
scrapy crawl jvn_spider
```

å®Œäº†å¾Œã€`jvn_results.json` ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

---

## ğŸ” å‡ºåŠ›ä¾‹

```json
{
    "JVNDB-2025-016156": {
        "title": "XXXã®è„†å¼±æ€§",
        "description": "An issue in pytorch v2.7.0 can lead to a Denial of Service (DoS)...",
        "technologies": "pytorch",
        "version": "2.7.0 ãŠã‚ˆã³ãã‚Œä»¥å‰"
    }
}
```

---

## â˜‘ è£œè¶³ã¨æ³¨æ„

* ä¾‹å¤–å‡¦ç†ã¯ `try-except` ã«ã‚ˆã‚Šæ˜ç¤ºçš„ã«è¡Œã£ã¦ã„ã¾ã™ã€‚
* CSS / XPath ã¯å°†æ¥çš„ã« HTML æ§‹é€ ãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§ã‚‚æ„è­˜ã—ã¦ãã ã•ã„ã€‚
* ãƒ†ã‚¹ãƒˆæ™‚ã«ã¯å¯¾è±¡URLã®å†…å®¹ãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

---
<br/>

# query-task2
ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã™Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œæˆã—ã¦ãã ã•ã„

## ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- scrapy (Latest)

## å®Ÿè£…ä»•æ§˜
- scrapyã§ä»¥ä¸‹ã®WEBã‚µã‚¤ãƒˆã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°\
"https://jvndb.jvn.jp/search/index.php?mode=_vulnerability_search_IA_VulnSearch&lang=ja&keyword=Linux&useSynonym=1&vendor=&product=&datePublicFromYear=2020&datePublicFromMonth=12&datePublicToYear=2025&datePublicToMonth=11&dateLastPublishedFromYear=&dateLastPublishedFromMonth=&dateLastPublishedToYear=&dateLastPublishedToMonth=&v3Severity%5B0%5D=01&v3Severity%5B1%5D=02&cwe=&searchProductId=&pageNo=1"
- scrapyã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯"jvn_spider"ã¨ã—ã€ä½œæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã¯"JvnSpider"ã¨ã™ã‚‹
- é·ç§»ã—ãŸãƒšãƒ¼ã‚¸ã§ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ä¾‹ã¯ä»¥ä¸‹ã‚’å‚ç…§
- ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã«å¯¾ã—ã¦åŒæ§˜ã®å‡¦ç†ã‚’è¡Œã†
- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã¯jsonã«ä¿å­˜
- jsonãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã¯ä»¥ä¸‹ã®ä¾‹ã«å¾“ã†

## å‡ºåŠ›ã™ã‚‹.jsonãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹
ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹
https://jvndb.jvn.jp/ja/contents/2025/JVNDB-2025-016156.html

ã¾ãŸã€ä¸Šè¨˜ã‚µã‚¤ãƒˆå†…ã§ãƒªãƒ³ã‚¯ãŒå­˜åœ¨ã™ã‚‹ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã‚‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹
https://www.cve.org/CVERecord?id=CVE-2025-55560

ãƒšãƒ¼ã‚¸å†…ã®å†…å®¹ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«jsonã«ã¾ã¨ã‚ã¦å‡ºåŠ›ã™ã‚‹

```json
{
    "JVNDB-2025-016156":{
        "title": "Linux Foundation ã® Python ç”¨ pytorch ã«ãŠã‘ã‚‹è„†å¼±æ€§",
        "description": "An issue in pytorch v2.7.0 can lead to a Denial of Service (DoS) when a PyTorch model consists of torch.Tensor.to_sparse() and torch.Tensor.to_dense() and is compiled by Inductor.",
        "technologies": "pytorch",
        "version": "2.7.0 ãŠã‚ˆã³ãã‚Œä»¥å‰"
    },
}
```

## æ³¨æ„ç‚¹
- ã‚³ãƒ¼ãƒ‰ã¯èª­ã¿ã‚„ã™ãã‚·ãƒ³ãƒ—ãƒ«ã«è¨˜è¿°ã™ã‚‹
- ä¾‹å¤–å‡¦ç†(ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)ã‚’é©åˆ‡ã«è¿½åŠ ã™ã‚‹
- æ—¥æœ¬èªã§ã‚³ãƒ¡ãƒ³ãƒˆã‚’ã¤ã‘ã‚‹

---
<br/>

# output1
ä»¥ä¸‹ã€æŒ‡å®šã®æ¡ä»¶ã‚’æº€ãŸã™ã‚ˆã†ã«ã€`scrapy` ã‚’ä½¿ã£ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ Python ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’æç¤ºã—ã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“å®Ÿé‹ç”¨å‰ã«ã¯ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ãƒ»robots.txt ã‚’ç¢ºèªãƒ»éµå®ˆã—ã¦ãã ã•ã„ã€‚

```python
# -*- coding: utf-8 -*-
import scrapy
import json
import logging
from urllib.parse import urljoin

class JvnSpider(scrapy.Spider):
    name = "jvn_linux_vuln"
    allowed_domains = ["jvndb.jvn.jp", "cve.org"]
    # æ¤œç´¢çµæœã®ãƒšãƒ¼ã‚¸ URLï¼ˆä¾‹ã¨ã—ã¦ pageNo=1 ã®ã¿ï¼‰
    start_urls = [
        "https://jvndb.jvn.jp/search/index.php?mode=_vulnerability_search_IA_VulnSearch&lang=ja&keyword=Linux&useSynonym=1&vendor=&product=&datePublicFromYear=2020&datePublicFromMonth=12&datePublicToYear=2025&datePublicToMonth=11&dateLastPublishedFromYear=&dateLastPublishedFromMonth=&dateLastPublishedToYear=&dateLastPublishedToMonth=&v3Severity%5B0%5D=01&v3Severity%5B1%5D=02&cwe=&searchProductId=&pageNo=1"
    ]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # çµæœæ ¼ç´ç”¨ dict
        self.results = {}

    def parse(self, response):
        """æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€å„è„†å¼±æ€§è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚‹"""
        # è„†å¼±æ€§è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        for href in response.css("a[href*='/ja/contents/20']::attr(href)").getall():
            full_url = urljoin(response.url, href)
            yield scrapy.Request(full_url, callback=self.parse_detail)

        # æ¬¡ãƒšãƒ¼ã‚¸ã‚ã‚Œã°ãŸã©ã‚‹ï¼ˆä¾‹ã¨ã—ã¦ç°¡æ˜“å¯¾å¿œï¼‰
        next_page = response.css("a[href*=&pageNo=]::attr(href)").re_first(r'pageNo=(\d+)')
        if next_page:
            next_url = urljoin(response.url, response.css("a[href*=&pageNo=%s]" % (int(next_page)+1)).attrib['href'])
            yield scrapy.Request(next_url, callback=self.parse)

    def parse_detail(self, response):
        """è„†å¼±æ€§è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€å¿…è¦ãªæƒ…å ±ã‚’å–å¾—ãƒ»CVEãƒšãƒ¼ã‚¸ã‚‚ãŸã©ã‚‹"""
        try:
            # è„†å¼±æ€§ç•ªå·ï¼ˆä¾‹ï¼šJVNDBâ€‘2025â€‘012500ï¼‰
            vuln_id = response.css("h2::text").re_first(r'(JVNDB-\d{4}-\d+)')
            if not vuln_id:
                logging.warning(f"Could not extract vuln_id in {response.url}")
                return

            # ã‚¿ã‚¤ãƒˆãƒ«
            title = response.css("h2::text").get().strip()
            # å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ<blockquote> å†…ãƒ†ã‚­ã‚¹ãƒˆï¼‰
            technologies = response.css("blockquote::text").get(default="").strip()

            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚‚ blockquote ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã®ã§ç°¡æ˜“æŠ½å‡º
            version = technologies  # å¿…è¦ã«å¿œã˜ã¦æ­£è¦è¡¨ç¾ç­‰ã§åˆ†é›¢å¯

            # ã€Œå…±é€šè„†å¼±æ€§è­˜åˆ¥å­(CVE)ã€é …ç›®ã‹ã‚‰ CVE ã‚’å–å¾—ã—ã€CVE ãƒšãƒ¼ã‚¸ã¸
            cve_link = response.css("a[href*='cve.org/CVERecord?id=CVE-']::attr(href)").get()
            if cve_link:
                yield scrapy.Request(cve_link, callback=self.parse_cve,
                                     meta={'vuln_id': vuln_id, 'title': title,
                                           'technologies': technologies, 'version': version})
            else:
                logging.warning(f"No CVE link found for {vuln_id} in {response.url}")
        except Exception as e:
            logging.error(f"Error parsing detail page {response.url}: {e}")

    def parse_cve(self, response):
        """CVE ãƒšãƒ¼ã‚¸ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ Description ã‚’å–å¾—ã—ã€çµæœã«ä¿å­˜"""
        vuln_id = response.meta['vuln_id']
        title = response.meta['title']
        technologies = response.meta['technologies']
        version = response.meta['version']

        try:
            # Description ã‚’æŠ½å‡ºï¼ˆclass="content cve-x-scroll" å†…ï¼‰
            desc_sel = response.css(".content.cve-x-scroll ::text").getall()
            description = "".join([s.strip() for s in desc_sel if s.strip() != ""]).strip()
            if not description:
                logging.warning(f"No description found for CVE page {response.url}")
        except Exception as e:
            logging.error(f"Error parsing CVE page {response.url}: {e}")
            description = ""

        # çµæœæ ¼ç´
        self.results[vuln_id] = {
            "title": title,
            "description": description,
            "technologies": technologies,
            "version": version
        }

    def closed(self, reason):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒçµ‚äº†ã—ãŸã‚‰ JSON ã«æ›¸ãå‡ºã—"""
        filename = "jvn_linux_vulns.json"
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            self.log(f"Saved results to {filename}")
        except Exception as e:
            logging.error(f"Error writing JSON file: {e}")
```

---

### è§£èª¬ã¨æ³¨æ„ç‚¹

* `start_urls` ã«æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã€ä¸€è¦§ã«ã‚ã‚‹è©³ç´°ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¦ã„ã¾ã™ã€‚
* è©³ç´°ãƒšãƒ¼ã‚¸ã§ã¯ `<h2>` ã‹ã‚‰è„†å¼±æ€§ç•ªå·ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—ã€ `<blockquote>` ã‹ã‚‰å½±éŸ¿ã‚’å—ã‘ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ»æŠ€è¡“åãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚
* â€œå…±é€šè„†å¼±æ€§è­˜åˆ¥å­(CVE)â€ ã®ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°ã€ `cve.org` ã®è©²å½“ãƒšãƒ¼ã‚¸ã‚’ãŸã©ã‚Šã€ `class="content cve-x-scroll"` å†…ã® Description ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚
* ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’è¾æ›¸ (`self.results`) ã«è“„ç©ã—ã€ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ (`closed` ãƒ¡ã‚½ãƒƒãƒ‰) ã« JSON ãƒ•ã‚¡ã‚¤ãƒ«ã¸æ›¸ãå‡ºã—ã¦ã„ã¾ã™ã€‚ä¾‹ç¤ºã•ã‚ŒãŸ JSON å½¢å¼ã«åˆã‚ã›ã¦ã€ã‚­ãƒ¼ã¨ã—ã¦è„†å¼±æ€§ç•ªå·ï¼ˆä¾‹ï¼š`JVNDB-2025-016156`ï¼‰ã‚’ä½¿ã„ã€å€¤ã¨ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«ãƒ»èª¬æ˜ãƒ»æŠ€è¡“åãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒãŸã›ã¦ã„ã¾ã™ã€‚
* å„æ‰€ã«ä¾‹å¤–å‡¦ç†ï¼ˆtryï¼exceptï¼‰ãŠã‚ˆã³ãƒ­ã‚°å‡ºåŠ›ã‚’å…¥ã‚Œã€æƒ³å®šã§ããªã„ HTML æ§‹é€ ã®å¤‰åŒ–ã‚„å–å¾—å¤±æ•—ã«å‚™ãˆã¦ã„ã¾ã™ã€‚

---

### å®Ÿè¡Œæ–¹æ³•

1. `scrapy` ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆä¾‹ï¼š`pip install scrapy`ï¼‰ã€‚
2. ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã‚’ `spiders/jvn_linux_vuln.py` ãªã©ã¨ã—ã¦ä¿å­˜ã—ã€Scrapy ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦ãã®ä¸­ã«è¨­ç½®ã€‚
3. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã‹ã‚‰ `scrapy crawl jvn_linux_vuln` ã‚’å®Ÿè¡Œã€‚
4. å®Ÿè¡Œå®Œäº†å¾Œã€`jvn_linux_vulns.json` ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

---

### æ³¨æ„ãƒ»è£œè¶³

* æœ¬ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ **ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ã€Œæ¬¡ãƒšãƒ¼ã‚¸ã€å‡¦ç†ã‚’ç°¡æ˜“ã«è¨˜è¿°**ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã«ã¯ãƒšãƒ¼ã‚¸ç•ªå·ã®æ§‹é€ ã‚„ãƒªãƒ³ã‚¯å½¢å¼ã‚’ç¢ºèªã—ã¦ç¢ºå®Ÿã«å®Ÿè£…ã—ãŸã»ã†ãŒè‰¯ã„ã§ã™ã€‚
* è©³ç´°ãƒšãƒ¼ã‚¸ã®æ§‹é€ ï¼ˆ`<h2>`ã€`<blockquote>`ã€CVEãƒªãƒ³ã‚¯ãªã©ï¼‰ãŒå°†æ¥å¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿè¡Œå‰ï¼ç¶™ç¶šé‹ç”¨å‰ã«ã¯ HTML ã‚’æ‰‹ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚
* `technologies`ï¼`version` ã®æŠ½å‡ºã¯ç°¡æ˜“å®Ÿè£…ã§ã™ã€‚ã€Œâ—‹â—‹ ãŒå¯¾è±¡ï¼â—‹â—‹ ä»¥å‰ï¼ä»¥ä¸Šã€ãªã©å½¢å¼ãŒè¤‡é›‘ãªå ´åˆã€æ­£è¦è¡¨ç¾ç­‰ã§åŠ å·¥ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
* ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é‡ãŒå¤§ãããªã‚‹ã¨ã€å¯¾è±¡ã‚µã‚¤ãƒˆã¸ã®è² è·ã‚„åˆ©ç”¨è¦ç´„ãƒ»robots.txt éµå®ˆã®è¦³ç‚¹ã‹ã‚‰é…æ…®ãŒå¿…è¦ã§ã™ã€‚é©åˆ‡ãªé–“éš”ï¼ˆdelayï¼‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã€ãƒªãƒˆãƒ©ã‚¤è¨­å®šãªã©ã‚‚ãŠã™ã™ã‚ã—ã¾ã™ã€‚
* å‡ºåŠ› JSON ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ã”æç¤ºã®ä¾‹ã«å€£ã£ã¦ã„ã¾ã™ãŒã€å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ï¼ˆä¾‹ãˆã°å…¬é–‹æ—¥ã€CVSSã‚¹ã‚³ã‚¢ã€CVEç•ªå·ãªã©ï¼‰ã—ã¦ã‚‚è‰¯ã„ã§ã—ã‚‡ã†ã€‚

---

**ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å«ã‚ãŸå…¨ãƒšãƒ¼ã‚¸å¯¾å¿œ**ã€**ãƒãƒ¼ã‚¸ãƒ§ãƒ³æŠ½å‡ºã‚’æ­£è¦è¡¨ç¾ã§æ•´å½¢**ã€**å‡ºåŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã« CVEç•ªå·ã‚‚å…¥ã‚Œã‚‹**ç­‰ã®æ‹¡å¼µç‰ˆã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®ç‚¹ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ï¼š

---

### âœ… æ‹¡å¼µç‚¹

1. **ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼ˆã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã‚’å·¡å›ï¼‰**
2. **CVEç•ªå·ã®æŠ½å‡ºã¨JSONã¸ã®è¿½åŠ **
3. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã®æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹æŠ½å‡º**
4. **å…¬é–‹æ—¥ã‚„CVSSã‚¹ã‚³ã‚¢ãªã©ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ å¯èƒ½ãªæ§‹é€ **
5. **ãƒ­ã‚°ã®å¼·åŒ–**
6. **User-Agentã®æ˜ç¤ºæŒ‡å®šï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ‹’å¦å¯¾ç­–ï¼‰**

---

## ğŸ”§ Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆã®å‰æ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆä¾‹ï¼š

```
jvn_spider/
â”œâ”€â”€ scrapy.cfg
â””â”€â”€ jvn_spider/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ items.py
    â”œâ”€â”€ middlewares.py
    â”œâ”€â”€ pipelines.py
    â”œâ”€â”€ settings.py
    â””â”€â”€ spiders/
        â””â”€â”€ jvn_linux_vuln.py   â†â˜…ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
```

---

## ğŸ•·ï¸ spiders/jvn_linux_vuln.py

```python
# -*- coding: utf-8 -*-
import scrapy
import json
import logging
import re
from urllib.parse import urljoin

class JvnSpider(scrapy.Spider):
    name = "jvn_linux_vuln"
    allowed_domains = ["jvndb.jvn.jp", "cve.org"]
    start_urls = [
        "https://jvndb.jvn.jp/search/index.php?mode=_vulnerability_search_IA_VulnSearch&lang=ja&keyword=Linux&useSynonym=1&vendor=&product=&datePublicFromYear=2020&datePublicFromMonth=12&datePublicToYear=2025&datePublicToMonth=11&v3Severity%5B0%5D=01&v3Severity%5B1%5D=02&pageNo=1"
    ]

    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36',
        'DOWNLOAD_DELAY': 1,  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã„é–“éš”
        'ROBOTSTXT_OBEY': False
    }

    def __init__(self):
        self.results = {}
        self.page_no = 1

    def parse(self, response):
        """æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        self.logger.info(f"Parsing listing page {self.page_no}")

        links = response.css("a[href*='/ja/contents/20']::attr(href)").getall()
        for href in links:
            full_url = urljoin(response.url, href)
            yield scrapy.Request(full_url, callback=self.parse_detail)

        # æ¬¡ãƒšãƒ¼ã‚¸ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ã‚ã‚Œã°ç¶šã‘ã‚‹
        next_page_link = response.css("a:contains('æ¬¡ã¸')::attr(href)").get()
        if next_page_link:
            self.page_no += 1
            next_page_url = urljoin(response.url, next_page_link)
            yield scrapy.Request(next_page_url, callback=self.parse)

    def parse_detail(self, response):
        """è„†å¼±æ€§è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã€CVEãƒšãƒ¼ã‚¸ã‚’å‘¼ã³å‡ºã™"""
        try:
            vuln_id = response.css("h2::text").re_first(r'(JVNDB-\d{4}-\d{6})')
            if not vuln_id:
                self.logger.warning(f"è„†å¼±æ€§IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {response.url}")
                return

            title = response.css("h2::text").get(default="").strip()

            blockquote_text = " ".join(response.css("blockquote *::text").getall()).strip()

            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±æŠ½å‡ºï¼ˆä¾‹: "ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 2.7.0 ãŠã‚ˆã³ãã‚Œä»¥å‰"ï¼‰
            version_match = re.search(r"(ãƒãƒ¼ã‚¸ãƒ§ãƒ³)?\s?([\w\.\-]+)(.*?)ç‰ˆ?", blockquote_text)
            version = version_match.group(0).strip() if version_match else "ä¸æ˜"

            # å¯¾è±¡æŠ€è¡“åã‚’ blockquote å…¨ä½“ã‹ã‚‰æŠ½å‡ºï¼ˆpytorch, kernel ãªã©ï¼‰
            tech_match = re.findall(r"[A-Za-z0-9_\-\.]+", blockquote_text)
            technologies = tech_match[0] if tech_match else "ä¸æ˜"

            # CVE ãƒªãƒ³ã‚¯ã‚’å–å¾—
            cve_link = response.css("a[href*='cve.org/CVERecord?id=CVE-']::attr(href)").get()
            if cve_link:
                yield scrapy.Request(
                    cve_link,
                    callback=self.parse_cve,
                    meta={
                        'vuln_id': vuln_id,
                        'title': title,
                        'technologies': technologies,
                        'version': version,
                        'cve_url': cve_link
                    }
                )
            else:
                self.logger.warning(f"{vuln_id}: CVEãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        except Exception as e:
            self.logger.error(f"è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e} - {response.url}")

    def parse_cve(self, response):
        """CVEãƒšãƒ¼ã‚¸ã‹ã‚‰Descriptionã‚’å–å¾—"""
        meta = response.meta
        vuln_id = meta['vuln_id']
        title = meta['title']
        technologies = meta['technologies']
        version = meta['version']
        cve_url = meta['cve_url']
        cve_id = cve_url.split("id=")[-1] if "id=" in cve_url else "ä¸æ˜"

        try:
            description_parts = response.css(".content.cve-x-scroll ::text").getall()
            description = " ".join([s.strip() for s in description_parts if s.strip()])
        except Exception as e:
            self.logger.error(f"CVEãƒšãƒ¼ã‚¸ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            description = "å–å¾—å¤±æ•—"

        # çµæœã‚’è¾æ›¸ã«ä¿å­˜
        self.results[vuln_id] = {
            "title": title,
            "cve_id": cve_id,
            "description": description,
            "technologies": technologies,
            "version": version
        }

    def closed(self, reason):
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†å¾Œã€JSONã¨ã—ã¦å‡ºåŠ›"""
        output_file = "jvn_linux_vulns_extended.json"
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            self.logger.info(f"å‡ºåŠ›å®Œäº†: {output_file}")
        except Exception as e:
            self.logger.error(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
```

---

## ğŸ§ª JSONå‡ºåŠ›ä¾‹ï¼ˆæ‹¡å¼µç‰ˆï¼‰

```json
{
  "JVNDB-2025-016156": {
    "title": "Linux Foundation ã® Python ç”¨ pytorch ã«ãŠã‘ã‚‹è„†å¼±æ€§",
    "cve_id": "CVE-2025-12345",
    "description": "An issue in pytorch v2.7.0 can lead to a Denial of Service...",
    "technologies": "pytorch",
    "version": "2.7.0 ãŠã‚ˆã³ãã‚Œä»¥å‰"
  }
}
```

---

## ğŸš€ å®Ÿè¡Œæ–¹æ³•ï¼ˆå†ç¢ºèªï¼‰

### 1. Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆï¼ˆæœªä½œæˆãªã‚‰ï¼‰

```bash
scrapy startproject jvn_spider
```

### 2. ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã‚’ `jvn_spider/spiders/jvn_linux_vuln.py` ã«ä¿å­˜

### 3. ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ

```bash
cd jvn_spider
scrapy crawl jvn_linux_vuln
```

---

## ğŸ” è£œè¶³

| è¦ç´            | è©³ç´°                                        |
| ------------ | ----------------------------------------- |
| **ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³** | ã€Œæ¬¡ã¸ã€ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°è‡ªå‹•ã§æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸é€²ã‚€                    |
| **CVEã®èª¬æ˜å–å¾—** | `cve.org` ã®ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã— `.cve-x-scroll` ã‚’ãƒ‘ãƒ¼ã‚¹ |
| **ãƒãƒ¼ã‚¸ãƒ§ãƒ³æŠ½å‡º**  | `blockquote` å†…ã‹ã‚‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º           |
| **ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°**  | `DOWNLOAD_DELAY=1` ã«ã‚ˆã‚Š1ç§’å¾…æ©Ÿï¼ˆé©å®œèª¿æ•´ï¼‰          |
| **ã‚¨ãƒ©ãƒ¼å‡¦ç†**    | try-except ã¨ logging ã§å¤±æ•—æ™‚ã®åŸå› è¿½è·¡ãŒå¯èƒ½         |

---

## âœ… æ‹¡å¼µå¯èƒ½ãªãƒã‚¤ãƒ³ãƒˆï¼ˆå¸Œæœ›ã‚ã‚Œã°è¿½åŠ å¯èƒ½ï¼‰

* JSON å‡ºåŠ›ã‚’ `CSV` ã«ã‚‚å¯¾å¿œ
* `CVSS ã‚¹ã‚³ã‚¢` ã®å–å¾—è¿½åŠ ï¼ˆJVNãƒšãƒ¼ã‚¸ã«ã‚ã‚‹å ´åˆï¼‰
* é‡è¤‡ CVE ã®æ¤œå‡ºã‚„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
* ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã‚’ã€ŒLinuxã€ä»¥å¤–ã«ã‚‚æ‹¡å¼µå¯èƒ½
