## 要約
我々は、キーバリューキャッシュに直接適用されるワンショット介入による言語モデルの暗黙的誘導を実現する軽量手法「キャッシュステアリング」を提案する。その有効性を検証するため、小規模言語モデルにおいて思考連鎖推論を誘導するためにキャッシュステアリングを適用した。本手法はGPT-4oが生成した推論トレースを活用し、微調整やプロンプト変更なしにモデルの挙動をより明示的で多段階の推論へと導く誘導ベクトルを構築する。多様な推論ベンチマークにおける実験評価により、キャッシュステアリングが
モデル推論の質的構造と定量的タスク性能の両方を向上させることが実証された。
継続的な介入を必要とする従来の活性化ステアリング技術と比較して、
我々のワンショットキャッシュステアリングは、ハイパーパラメータの安定性、
推論時間の効率性、統合の容易性の面で大きな利点を提供し、
制御された生成のためのより堅牢で実用的な解決策となる。

## はじめに
大規模言語モデルが複雑な推論を実行する能力は、その有用性が高まる主要な要因である。しかし、この潜在能力は常に自発的に発揮されるわけではなく、特に潜在的な推論能力を有しながらも特定の誘導を必要とする小規模モデルでは顕在化しない。教師あり微調整や思考連鎖例を用いた少例学習プロンプティングといった従来の能力発掘手法は有効だが、膨大なデータや複雑なプロンプト設計を要することが多い。そこで疑問が生じる：トレーニング後にこれらの固有推論プロセスを解き放ち誘導する、より軽量な介入手法を開発できるだろうか？

有望な方向性の一つが活性化誘導（Turner et al., 2024; Panickssery et al., 2023）である。これはモデルの内部隠れ状態を直接変更することで、その挙動を導くことを目的とする。再学習なしで出力に影響を与え得る点で有望である一方、活性化誘導は効果を発揮するためにデコードプロセス全体を通じて各トークン生成ステップで継続的な介入を必要とする場合が多い（Wehner et al., 2025）。この継続的な操作は不安定性を生じさせ、結果を超パラメータの選択（対象とする層、介入強度など）に極めて敏感にし、生成品質の低下を招く可能性がある。

これらの課題に対処するため、我々は「キャッシュステアリング」と呼ばれる手法を導入する。本アプローチは、Transformerモデルのキーバリューキャッシュに対して直接、標的を絞った単発の修正を行うことで動作する。
通常、キャッシュは最初のプロンプトによって初期化されます。GPT-4oのような高性能な教師モデルが生成した推論トレースから導出されたステアリングベクトルを、これらのキャッシュされたキーと値の表現に適用することで、小型モデルの推論軌道を誘導できます。この単一の介入は、トークン生成開始前に適用されるため、モデル重みを変更したり複雑なプロンプト修正を必要とせずに、より明示的で多段階の推論へと効果的に誘導します。

これにより以下の主要な利点が得られる：
ハイパーパラメータ変動に対する安定性と頑健性の向上、
継続的な介入が不要なため推論時の計算オーバーヘッドの大幅削減、
標準的なトランスフォーマー推論パイプラインとのシームレスな統合。
我々の手法は推論構造を改善し、GSM8K、ARC-Challenge、CSQA、PIQAを含む
複数のベンチマークにおいて多くのケースでタスク精度を向上させることを実証する。
全体として、我々の主な貢献は以下の通りである：
• キャッシュステアリングを提案する。これは推論時にKVキャッシュをワンショットで変更することでモデル挙動を制御する新規技術である。
• キャッシュステアリングにより、GPT-4oなどの大規模モデルから推論スタイルを蒸留し、
微調整やプロンプト拡張なしに小型モデルへ転移できることを実証。
• 推論ベンチマークで広範な評価を実施し、活性化ステアリングやCoTプロンプティングと比較。
効率性と安定性を分析。

## 関連研究
### 推論と思考の連鎖プロンプティング。
LLMの推論能力を強化する広く採用されている手法には、言語モデルへのプロンプト内で段階的な推論プロセス（思考の連鎖：CoT）を含む問題の解決例を示す方法（文脈内学習：ICL）がある（Brown, 2020; Wei et al., 2022）。この手法は「少例学習プロンプティング」として知られる。
CoTプロンプティングのゼロショット変種は、「段階的に考えよう」といった指示を追加することで、
例示の提示を必要とせずに段階的推論を誘導し、このアプローチを簡略化する（Kojima et al., 2022）。
最近の研究では、強化学習が顕著な推論能力をもたらし、
教師あり微調整を通じて小型モデルへ効果的に蒸留できることが示されている（Guo et al., 2025）。
これらの知見は、言語モデルでCoT推論を単に誘発するだけでは不十分であり、推論の様式が重要であることを示唆している。これが我々のアプローチの動機であり、キャッシュレベルでの介入を通じて、大規模な教師モデルを彷彿とさせる推論行動へ小型モデルを直接誘導することを目指す。

### 活性化ステアリング。
活性化ステアリング（別名：表現工学）は、通常は線形介入を通じて、デコード中の中間活性化を操作することでLLMの生成プロセスを暗黙的に制御する技術である（Panickssery et al., 2023; Turner et al., 2024）。複数の研究が、再学習なしにモデルに特定の行動を誘導または抑制するために活性化ステアリングを適用している。例としては、感情・トピック・スタイル制御（Turner et al., 2024）、機能ステアリング（Todd et al., 2023; Postmus and Abreu, 2024）、拒否行動の除去または誘導、 （Lee et al., 2024）、有害性の低減（Turner et al., 2024）、真実性（Wang et al., 2025）、
事実知識の編集（Yin et al., 2024）、推論（Zhang and Viteri, 2025; Galichin et al., 2025)
その他（Wehner et al., 2025）などである。
最も基本的な形態において、活性化ステアリングは2つのステップを含む：ベクトル抽出と推論時のモデル活性化へのベクトル注入。ベクトル抽出段階では「ステアリングベクトル」を計算する。これは通常、望ましい挙動を持つ肯定的プロンプトと否定的あるいは中立的なプロンプトのペアから活性化を集約することで行われ、対照セットt$C = {(p^+_0, p^−_0),(p^+_1, p^−_1), ...,(p^+_N , p^−_N)}$を形成する。最も一般的な集約手法はDifferencein-Means（Wehner et al., 2025）であり、ベクトルがペア化されている場合にはMean-of-Differencesと同一となる：
$$s_l = \frac{1}{N} \sum_{(p^+,p^−)∈C} f_l(p^+) − f_l(p^−)$$

ここで、fl は層 l における Transformer モデルの一部（例：デコーダ層全体）を表し、
N は対照データセット C 内の例の数である。モデルの出力の方向を制御するため、推論中に特定の層の活性化値にステアリングベクトルを加算する：
$$h^∗_l = h_l + c s_l$$
ここで hl はステアリング前の層 l における活性化値、sl は層 l から抽出されたステアリングベクトル、c はステアリングの強さを決定する係数である。
重要な点として、このベクトルは異なるトークン位置、層、モデル部分に対して抽出・適用可能であり、これらはハイパーパラメータまたは設計上の選択として扱われる。通常、ステアリングを適用する層とステアリング強度係数 c の値を決定するにはグリッド検索を行うのが一般的である（Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024）。
活性化ステアリングはモデル制御の手段を提供する一方で、生成中に継続的な介入を必要とするのが一般的である（Wehner et al., 2025）。これはコストが高く、不安定な生成を引き起こす可能性がある。これに対し、我々の研究は活性化ステアリングの原理をキーバリュー（KV）キャッシュに拡張し、推論時に効率的かつ安定性の高いワンショット介入を可能にする。

### キャッシュ操作。
別の新たな研究分野では、メモリと効率性の観点からキーバリュー（KV）キャッシュを改変するアイデアが探求されている（Li et al., 2024; Liu et al., 2024a; Ge et al., 2023; Mu et al., 2023）。これらのアプローチは、KVキャッシュ操作を通じてメモリフットプリントの削減や文脈表現の圧縮を目指す。この概念を発展させ、Liu et al. (2024b)は推論能力を必要とするタスクの性能向上のためにKVキャッシュを拡張する手法を提案した。著者らは微分可能な「コプロセッサ」を用い、フォワードパス中に活性化を直接変更する代わりに、生成前のステップとしてKVキャッシュを拡張することを可能にしている。
しかしながら、KVキャッシュを拡張するためには、別途モデルを訓練する必要があり、
この手法は前のサブセクションで紹介した純粋な活性化ステアリング手法よりも実用性に劣る。
これに対し、我々のアプローチは補助モジュールを訓練することなく、
小規模モデルにおいて行動制御のターゲットとしてKVキャッシュを利用することを目指す。

### 予備知識
トランスフォーマーベースの言語モデルは、クエリ、キー、値のベクトル集合に対して動作する自己注意機構に依存し、文脈化されたトークン表現を計算する。与えられた入力シーケンスに対して、層 l における注意出力は以下のように計算される：
Attention(Q^l, K^l,V^l) = softmax Q^l (K^l)^⊤ √Dh! V^l
ここで、Q^l, K^l,V^l ∈ R^{T×H×D_h}は層 l におけるクエリ、キー、値テンソルであり、T はシーケンス長、H はアテンションヘッド数、Dh は各ヘッドの次元数である。
自己回帰的デコード中、モデルは処理済みトークンに対応するキー K^l と値 V^l を保存する。これはキー・バリュー（KV）キャッシュとして知られる。これらのキャッシュされたテンソルは、シーケンス全体の表現を再計算することなく、各新規トークンに対する注意を効率的に計算するために使用される。重要な点として、これらのキャッシュエントリは事前計算され、複数の例（システムプロンプトのキャッシュなど）で再利用可能である。これは大規模モデルや類似入力に対する反復推論を伴うシナリオで特に有用である。これによりKVキャッシュは行動介入の潜在的な対象となり、実世界設定との互換性を提供する。

### KVステアリングベクトルの抽出
活性化ステアリングと同様に、対照的なプロンプトペアの集合 C ={(p^+_0, p^−_0),(p^+_1, p^−_1), ...,(p^+_N, p^−_N )} を構築し、キー値ステアリングベクトルを抽出する。望ましい挙動を示すプロンプトを正例、該当しないプロンプトを負例と呼ぶ。
推論誘導タスクにおける正例・負例の構築詳細については
セクション3.5で議論する。
対照的な例ペアごとに、順伝播を行い、指定されたトークン位置（通常は入力プロンプトの最終トークン）からキーと値のベクトルを抽出する。その後、差の平均法を用いてベクトルを集約する：
S^k_l =1/N \sum_{(p+,p−)∈C} f_l(p^+) − f_l (p^−)
S^v_l =1/N \sum_{(p+,p−)∈C} f_l(p^+) − f_l (p^−)
ここで、f_l はトランスフォーマー層であり、S^k_l ∈ R^{H×D_h} および S^v_l ∈ R^{H×Dh} は層 l における結果のステアリングテンソルであり、H はアテンションヘッドの数を、Dh はその次元を表す。正例と負例の差分を取り、複数の対比ペア間で平均化することで、個々の例から生じる情報のノイズを最小化しつつ、目標行動に関連する方向性信号を抽出することを目指す。

### KVステアリングベクトルの適用
推論時、入力プロンプトに対して標準的な順伝播を実行し、KVキャッシュを埋める。
その後、各層 l において、KVキャッシュ内のターゲットトークン位置に対応するキャッシュ済みキーベクトルと値ベクトルを以下のように修正する：
V^∗_l = V_l + c^v S^v_l
K^∗_l = K_l + c^k S^k_l
ここで、K_l, V_l ∈ R^{H×D_h} は層 l における元のキャッシュ済みキー・値ベクトルであり、S^k_l, S^v_l ∈R^{H×D_h} はステアリングベクトル、c^k, c^v ∈ R はステアリング強度を制御するスカラー係数である。
その後、修正されたキャッシュを用いて通常通り生成を継続する。

### 直観
キャッシュステアリングは、生成過程におけるモデルの挙動に影響を与える方法とタイミングにおいて、従来の活性化ステアリングと根本的に異なる。主な相違点は図1に視覚的に示されている。以下に、本手法の核心となる直観を概説する。

1. クエリの変更なし。
活性化ステアリングは通常、クエリ・キー・値を生成する隠れ状態を変更する。クエリの変更は入力条件付け能力を低下させる可能性がある。これに対し、キャッシュステアリングは過去のトークンから保存されたキーと値のみを変更し、クエリは変更しない。これによりモデルはデコード時に入力文脈のより正確な表現を維持できる。
2. 過去を制御する vs 現在を制御する。
特定の時間ステップ t において、活性化制御は明示的に層 l の現在の隠れ状態に影響を与え、それがその後、モデル内の層数 N までの全ての後続層 l+1 から l+N へと伝播する。これは「オーバーステアリング」として知られる、層間で小さな介入が複合的に作用する連鎖効果を引き起こす可能性があり、介入後の生成品質に悪影響を及ぼすことがある。この特性により、活性化ステアリングはステアリング強度や適用層といった特定のハイパーパラメータに極めて敏感であることが広く実証されている。対照的に、キャッシュステアリングは過去のトークンの固定されたキーと値の表現を修正する。これらの値は層を通ってさらに処理されるのではなく、将来のクエリによって直接アテンションされる。つまり、キャッシュステアリングは効果の集約なしに多くの（あるいは全ての）層に適用可能であり、将来の隠れ状態に暗黙的な影響を与える。
3. ソース行動との整合性。
重要な点として、我々のステアリングベクトルはCoT推論トレースを含むプロンプトのキーと値から抽出される。標準的な少例学習プロンプティングシナリオでは、これらのキー値ペアは過去のキャッシュに存在し、生成時にアテンションされる。推論時にKVキャッシュにステアリングベクトルを適用することで、例を明示的に含めることなくそのスタイルを転移できることを期待している。

### 実装の詳細
対照的セット構築。ステアリングベクトルを抽出するため、対照的データセットを構築する。これはペアになったプロンプトで構成される。各ペアには、肯定例（明示的な思考の連鎖を含む）と否定例（最終回答のみを含む）が含まれる。各対照プロンプトは、少例学習（ICL）例を用いて作成される。具体的には、肯定例と否定例の両方に、n個のICL例とそれに続く質問および生成プロンプトが含まれる。肯定例と否定例の違いは、ICL例に推論ステップが含まれるか否かのみである（詳細は付録Bおよび例を参照）。

抽出と適用位置。プロンプトの最終トークンからキーと値のベクトルを抽出する。これはモデルのチャットテンプレートに依存するが、通常は生成プロンプトの最終トークンに対応する（例：「assistant\n\n」内の「\n\n」）。推論時には、抽出時に使用したプロンプト内の同一論理位置にキャッシュステアリングを適用することを目指す。ただし、自己回帰的デコード機構（3.1節参照）により、KVキャッシュは各トークン処理後にのみ更新される。位置合わせを確保するため、プロンプトに中立オフセットトークン（改行や空白など）を追加する。これにより最終トークンのKV表現を次のトークン生成に利用可能となり、キャッシュステアリングが意図した位置に影響することを保証する。トークン位置合わせとキャッシュオフセットの詳細は付録Fに記載する。

ハイパーパラメータ。活性化ステアリングと同様に、キーと値のベクトルのステアリング強度係数は
ハイパーパラメータとして扱われます。より大きなモデルから推論行動を蒸留することに関心があるため、
対照ペアの数と各ペアの文脈内例の数も追加のハイパーパラメータとして扱います。
他のステアリング手法と同様に、 他のステアリング手法と同様に、
各モデルとデータセットのペアについて、ハイパーパラメータに対して小規模なグリッド検索を行い、妥当な値を取得します。ステアリング係数は、タスク間で一貫した範囲内に収まる傾向があり、この手法の挙動の頑健性を示唆していることがわかりました。
これについては、セクション 5.2 で詳しく説明します。ハイパーパラメータの完全なリストは、
付録 G に記載されています。

---
DeepL.com（無料版）で翻訳しました。